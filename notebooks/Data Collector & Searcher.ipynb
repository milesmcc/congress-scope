{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import hashlib\n",
    "import random\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import unicodecsv as csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def md5hash(item):\n",
    "    m = hashlib.md5()\n",
    "    m.update(json.dumps(item[\"metadata\"][\"id\"]).encode('ascii'))\n",
    "    m.update(json.dumps(item[\"text\"]).encode('ascii'))\n",
    "    m.update(json.dumps(item[\"metadata\"][\"header\"]).encode('ascii'))\n",
    "    return m.hexdigest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for root, dirs, files in os.walk('../data/'):\n",
    "    for file in files:\n",
    "        path = os.path.join(root, file)\n",
    "        if path.endswith(\".json\") and (\"/110/\" in path or \"/111/\" in path):\n",
    "            with open(path, \"r\") as infile:\n",
    "                data = json.load(infile)\n",
    "                congress = int(re.search(r\"data\\/(\\d+)\\/output\", path).group(1))\n",
    "                metadata = {\n",
    "                    \"header\": data[\"header\"],\n",
    "                    \"id\": data[\"id\"],\n",
    "                    \"title\": data[\"title\"],\n",
    "                    \"document_title\": data[\"doc_title\"],\n",
    "                    \"congress\": congress\n",
    "                }\n",
    "                for item in data[\"content\"]:\n",
    "                    item[\"metadata\"] = metadata\n",
    "                    item[\"hash\"] = md5hash(item)\n",
    "                    items.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/corpus.json\", \"w\") as outfile:\n",
    "    json.dump(items, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Searches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_search_results(items, file):\n",
    "    with open(file, \"wb\") as outfile:\n",
    "        writer = csv.writer(outfile)\n",
    "        writer.writerow([\"HASH\", \"SPEAKER\", \"BIOGUIDE\", \"DATE\", \"TITLE\", \"TEXT\"])\n",
    "        for item in items:\n",
    "            text = item[\"text\"]\n",
    "            date = \"-\".join([item[\"metadata\"][\"header\"][\"day\"], item[\"metadata\"][\"header\"][\"month\"], item[\"metadata\"][\"header\"][\"year\"]])\n",
    "            writer.writerow([item[\"hash\"], item[\"speaker\"], item[\"speaker_bioguide\"], date, item[\"metadata\"][\"title\"], item[\"text\"]])\n",
    "            \n",
    "def write_sample(items, file):\n",
    "    with open(file, \"wb\") as outfile:\n",
    "        writer = csv.writer(outfile)\n",
    "        writer.writerow([\"ID\", \"RELEVANT\"])\n",
    "        for item in items:\n",
    "            writer.writerow([item[\"hash\"], \"\"])\n",
    "            \n",
    "def write_markdown(items, title, file):\n",
    "    with open(file, \"w\") as outfile:\n",
    "        outfile.write(\"%% %s\\n%% %s Total Records | %s\" % (title, str(len(items)), str(datetime.now().date())))\n",
    "        for item in items:\n",
    "            outfile.write(\"\\n\\n---\\n\\n\")\n",
    "            date = \" \".join([item[\"metadata\"][\"header\"][\"day\"], item[\"metadata\"][\"header\"][\"month\"], item[\"metadata\"][\"header\"][\"year\"]])\n",
    "            outfile.write(\"## %s\\n\" % item[\"metadata\"][\"title\"])\n",
    "            outfile.write(\"## `%s`\\n\" % item[\"hash\"])\n",
    "            outfile.write(\"`%s — %s`\\n\\n\" % (item[\"speaker\"], date))\n",
    "            text = item[\"text\"].replace(\"  \", \"\\n\").replace(\"`\", \"'\").replace(\"\\\\\", \"/\")\n",
    "            outfile.write(text)\n",
    "            \n",
    "def write_website(items, directory):\n",
    "    with open(directory + \"README.md\", \"w\") as outfile:\n",
    "        outfile.write(\"---\\n---\\n\\n# Congressional Record Index\\n\")\n",
    "        outfile.write(\"This website provides a temporary interface for accessing Congressional Record search results.\\nTo find a particular post, use `Cmd+F` and search for either its title or its ID.\\n\\n\")\n",
    "        outfile.write(\"---\\n\\n\")\n",
    "        for item in items:\n",
    "            outfile.write(\"* [%s (%s)](%s.md)\\n\" % (item[\"metadata\"][\"title\"], item[\"hash\"], item[\"hash\"]))\n",
    "    for item in tqdm(items):\n",
    "        with open(directory + \"%s.md\" % item[\"hash\"], \"w\") as outfile:\n",
    "            date = \" \".join([item[\"metadata\"][\"header\"][\"day\"], item[\"metadata\"][\"header\"][\"month\"], item[\"metadata\"][\"header\"][\"year\"]])\n",
    "            outfile.write(\"---\\nlayout: default\\n---\\n\\n# %s\\n\" % item[\"metadata\"][\"title\"])\n",
    "            outfile.write(\"## `%s`\\n\" % item[\"hash\"])\n",
    "            outfile.write(\"`%s — %s`\\n\\n\" % (item[\"speaker\"], date))\n",
    "            outfile.write(\"---\\n\\n\")\n",
    "            text = item[\"text\"].replace(\"  \", \"\\n\").replace(\"`\", \"'\").replace(\"\\\\\", \"/\")\n",
    "            outfile.write(text)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_items = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iraq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_statement(item):\n",
    "    if \"speaker_bioguide\" not in item:\n",
    "        item[\"speaker_bioguide\"] = None\n",
    "    return not (str(item[\"speaker_bioguide\"]) == \"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 999390/999390 [00:01<00:00, 587025.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search returned 10216 results.\n"
     ]
    }
   ],
   "source": [
    "matched_items = []\n",
    "for item in tqdm(items):\n",
    "    if not is_statement(item):\n",
    "        continue\n",
    "    if \"speaker_bioguide\" not in item:\n",
    "        item[\"speaker_bioguide\"] = None\n",
    "    if \"iraq\" in item[\"text\"].lower() and item[\"metadata\"][\"congress\"] == 110:\n",
    "        matched_items.append(item)\n",
    "with open(\"../data/iraq.json\", \"w\") as outfile:\n",
    "    json.dump(matched_items, outfile)\n",
    "write_search_results(matched_items, \"../data/iraq.csv\")\n",
    "write_sample(random.sample(matched_items, 500), \"../data/iraq_sample.csv\")\n",
    "write_markdown(matched_items, \"Iraq Dataset\", \"../data/iraq.md\")\n",
    "all_items.extend(matched_items)\n",
    "print(\"Search returned %s results.\" % len(matched_items))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Afghanistan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 999390/999390 [00:01<00:00, 536784.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search returned 3539 results.\n"
     ]
    }
   ],
   "source": [
    "matched_items = []\n",
    "for item in tqdm(items):\n",
    "    if not is_statement(item):\n",
    "        continue\n",
    "    if \"speaker_bioguide\" not in item:\n",
    "        item[\"speaker_bioguide\"] == None\n",
    "    if \"afghanistan\" in item[\"text\"].lower() and item[\"metadata\"][\"congress\"] == 111:\n",
    "        matched_items.append(item)\n",
    "with open(\"../data/afghanistan.json\", \"w\") as outfile:\n",
    "    json.dump(matched_items, outfile)\n",
    "write_search_results(matched_items, \"../data/afghanistan.csv\")\n",
    "write_sample(random.sample(matched_items, 500), \"../data/afghanistan_sample.csv\")\n",
    "write_markdown(matched_items, \"Afghanistan Dataset\", \"../data/afghanistan.md\")\n",
    "all_items.extend(matched_items)\n",
    "print(\"Search returned %s results.\" % len(matched_items))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_markdown(all_items, \"Iraq and Afghanistan Dataset\", \"../data/all.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13755/13755 [00:01<00:00, 8137.38it/s]\n"
     ]
    }
   ],
   "source": [
    "write_website(all_items, \"../docs/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
